Ao realizarmos qualquer produto este tem que passar por uma série de etapas de modo a obtermos no final o melhor produto possível. Existe então um ciclo iterativo que devemos respeitar para isto acontecer.

![[Pasted image 20230314001407.png]]

Para avaliar o trabalho usamos membros exteriores aos do grupo. Pretendemos ==__avaliar e identifcar problemas__== de duas componentes:

- Usabilidade
- Experiência de Utilização

## Métodos de Avaliação

Realizam-se testes ==analíticos e empíricos.==

### Testes Analíticos

> [!EXAMPLE] DEFINIÇÃO
> Referem-se a avaliações heurísticas. Testes que envolvem apenas o trabalho e não as pessoas. Está associada a Lei de Fitts e Machine Learning.


### Testes Empíricos

> [!EXAMPLE] DEFINIÇÃO
> Referem-se a testes de usabilidade, testes A/B, diários, shadowing, context inquiry, etc. Aqui já importam a experiência do utilizador para saber o que é preferido para ser utilizado.
> 
> Podemos dividir as observações em duas: __Observações diretas__ em que observamos os utilizadores durante a execução seja presencial ou em vídeo.
> As __observações indiretas__ podem ser manuais (uso de diários) ou automáticos (através de registo de interação).
> 

#### Como usamos um diário?

Diários podem ser utilizados para ver a utilização a longo prazo de um produto.
(1-2 semanas). Os diários permitem ao utilizador externo partilhar os seus pensamentos, utilidade e relação em vez de ser apenas a interação com o produto.
O nosso objetivo é criar um produto que incorpore as atividades do dia-a-dia, e os diários são a melhor forma de avaliar isso mesmo.


# Case Study: Halo 3

Com o Halo 3 a Microsoft gerou um registo de interação automático. Tinham milhares de jogares a jogar e a avaliar a qualidade.
O que era registado:

-   Timestamp
-   Localização do jogador
-   Número de balas
-   Eventos (morrer, acertar, etc.)
-   Para onde está a apontar

A avaliar as condições a Microsoft criou o __Heatmap__ que a cada 5s mudava a cor do jogador. Se houvesse uma grande concentração de cores num mapa sabia-se que essa parte do mapa tinha que ser melhorada e se estivesse mais disperso estava bom.

Viam também as munições. O registo automático já verificava quando é que as balas eram todas perdidas mas foi necessário um resgisto direto para perceber o porquê dos jogares perderem todas as balas.

Hoje em dia pode lançar-se um jogo e através de feedback ao longo do tempo vai-se melhorando não sendo necessário esperar 5 anos para lançar um jogo.


## Diferentes Métodos de avaliação

-   **Entrevistas**
-   Focus groups
-   Questionários online
-   Diários
-   Sondas culturais
-   **Observação**
-   Etnografia
-   Experience sampling
-   **Registo de interação**
-   Co-desenho
-   Casos de estudo
-   Contextual inquiry
-   **Wizard of Oz**
-   **Think-aloud**
-   Card sorting
-   **...**


## Testes de Usabilidade


> [!EXAMPLE] DEFINIÇÃO
> Garantem que os produtos são bons e práticos. Permitem identificar problemas, descobrir onde melhorar e compreender os utilizadores através das suas preferências.

![[Pasted image 20230314020512.png]]

Para o 1º Bake-off focámo-nos na ==avaliação formativa== e no segundo na ==avaliação final ou sumativa==.

___==Avaliação Formativa==___ - Realiza-se durante o desenho, os resultados informam a próxima fase do desenho. ( O que acontece durante a interação ).

___==Avaliação Sumativa==___ - Refere-se à avaliação final. 
( Qual o resultado )

Para realizar um bom teste de usabilidade temos que nos focar em três parâmetros diferentes:

- Participantes
- Medidas
- Tarefas

### Participantes / Utilizadores

Partcicipantes são aqueles mais próximos do público alvo, com maior disponibilidade ( mais adequados à situação ). Homens e mulheres de todas as idades.

Para vermos a descrição do grupo de pessoas que realizam os testes temos que ver a sua Demographic Info.

SEGUNDO NIELSEN, devem ser 20 utilizadores, 4 iterações com 5 utilizadores cada.

Se tivermos:

-   1 utilizador → 33% dos problemas identificados
-   5 utilizadores → 85% dos problemas identificados
-   15 utilizadores → 99% dos problemas identificados

É necessário ter um número grande de utilizadores para ter a certeza que não há uma média curta e manipulável.

### Tarefas

Tarefas que apresentamos têm que ser bastante bem definidas.
Têm que ser:

-   **Reais** e representativas
-   **O quê** e não como
-   **Específicas**
-   Mistura de Complexidades
-   **Avaliação Comparativa**
    -   não favorecer uma das soluções
    -   usar as mesmas tarefas

Não podemos então dar soluções como oferecer um rato melhor ou mencionar que custou muito trabalho certa parte.

### Medidas de Usabilidade

-   Tempo para completar a tarefa
-   Número de erros cometidos
-   Número de tarefas concluídas
-   Número de cliques
-   Número de consultas à ajuda
-   Satisfação do utilizador

### Tipos de Dados

##### Quantitativos
(quantidade, específicos e medíveis)

-   **Completou a tarefa?** Sim/Não
-   **Quanto tempo demorou?**
-   **Quantos erros?**
-   **Qual preferiu?** A ou B

##### Qualitativo
(qualidade, aberto)

-   O que gostou mais na sua experiência?
-   O que pensa do ecrã principal?
-   Mais difícil de obter?

##### Objetivos

-   Não dependem da pré-disposição (bias) inerente ao ser humano (ex.: tempo, erros, frequência cardíaca, etc.)

##### Subjetivos

-   Realça a perceção do utilizador (ex.: preferência, **SUS**, **SEQ**, etc.)

**SUS:** acima de 68% é minimamente utilizável, já há valores médios por ser tão utilizado.
**SEQ:** debriefing se tiveram dificuldade a completar alguma tarefa.

### Testes-Piloto

2 a 3 pessoas para testar:

-   Duração
-   Instruções
-   Tarefas
-   Questionário

Estes testes ajudam a encontrar últimos erros antes de sair

### Testes A/B

Melhor tipo de testes que verifica qual é o melhor tipo de cor, logotipo, layout de página, tipografia, botões, etc.


## Google 41 shades of blue

A google mostrou a utilizadores 41 tons de azul e ao escolherem a empresa faturou. Ouve conflitos por terem sido os utilizadores e não as equipas profissionais a decidir isso.

### Como dividir os grupos?

___==Intergrupos==___ - Recrutar o dobro das pessoas e grupos comparáveis.
(Um grupo tem a vacina e o outro não.)

___==Intragrupos==___ - Cada grupo testa as duas interações. Vamos trocando a ordem de uso dos sistemas. (aprendizagem, fadiga).

Contudo há fatores que influenciam as pessoas e um grupo que já tenha testado 40x um projeto pode estar viciado ou cansado e isso influencia a avaliação.

Para combater usamos counter-balacing em que se tivermos os projetos [A, B, C]:
User1: ABC
User 2: ACB
...

Counter-balancing funciona de uma forma fatorial, resumidamente:

-   3 sistemas: 3! = 6 utilizadores
-   4 sistemas: 4! = 24 utilizadores
-   5 sistemas: 5! = 120 utilizadores

___==**Variáveis Dependentes:**==___

-   O seu valor depende do sistema a testar
-   Variáveis medidas no estudo (tempo, erros, SUS)
-   Relacionados com o objetivo do protótipo

___==**Variáveis Independentes:**==___

-   Não dependem das variáveis que estamos a medir
-   Características da solução (layout, cor, etc.)
-   Características dos participantes (idade, etc.)


## Efeitos Secundários

Aqui deparamos com um problemas de Fixação Funcional.
Dunker colocou à frente do seus participantes uma vela, um caixa de pioneses e fósfororos e pediu que arranjassem uma maneira de que a vela fosse acendida mas não pingasse cera para a mesa. Dividiu em 2 grupos:

- Grupo A tinha pioneses dentro da caixa.
- Grupo B tinha pioneses fora da caixa.

Como os pioneses estavam fora da caixa era mais evidente para o grupo B que a caixa também podia ser usada como elemento e assim este grupo terminou a experiência mais rápido.


Mais tarde Glucksberg testou a mesma experiência mas adicionou a variável de obter o melhor tempo de resolução e recompensas para o top 40% e o mais rápido.
Aqui os que tinham os pioneses na caixa precisavam de mais criatividade e foi mais lento. Os que tinham os pioneses dentro da caixa foram mais rápidos.

Adicionar uma recompensa aumenta o foco mas diminui a criatividade. É bom para instruções simples para tarefas claras.


## Efeito de Hawthorne / Efeito do Observador


> [!TIP] DEFINIÇÃO
> Quanto mais pressão, mais produtividade.
> 
> ___Novely Effect___ - quando vemos algo novo ficamos excitados sem razão. Pode até não ser nada de especial mas por ser novo temos curiosidade. Por isso temos que testar o produto mais do que uma vez para testar o quão prático e bom.


## Ética com Utilizadores

Quando pensamos em experiências pensamos sempre pelo senso comum que é tudo bem intencionado mas já aconteceu participantes ficarem traumatizados após os testes.
Um exemplo (Standford Experiment) é um teste em que se dividiu dois grupos: um com polícias e outro com prisioneiros. Ao fim de um tempo o poder subio aos polícias e começaram a maltratar os outros.

Portanto para fazermos qualquer teste temos que ter algumas considerações éticas.

### Considerações Éticas

É da responsabilidade de quem está a realizar o teste de aliviar e relaxar o participante de modo a que o teste não seja desconfortável ou stressante.

Consentimento é de extrema importância já que estamos a trabalhar com voluntários. Evitar presão. Informar os participantes que podem terminar a qualquer momento. Realçar que o sistema é que está a ser testado e nunca os utilizadores. Tornar os dados anónimos e proteger esses dados.

Existem pessoas vulneráveis.

Qualquer teste tem que ter uma aprovação da comissão ética para ser realizado.

### Três princípios de investigações com pessoas

1.  É necessário haver respeito pelas pessoas:

-   Os indivíduos têm autonomia e escolha
-   Não podem ser usados como meio para atingir um fim
-   É necessário proteger os mais vulneráveis
-   É necessário usar consentimento informado

2.  Beneficência _(fazer o bem)_:

-   Obrigação de fazer o bem
-   Obrigação de não prejudicar
-   Obrigação de prevenir danos
-   Minimizar riscos, maximizar benefícios
-   Bondade além do dever
-   Avaliação de riscos e benefícios

3.  Justiça:

-   Garantir igualdade
-   Dividir riscos e benefícios pela amostra
-   Recrutamento de participantes justo